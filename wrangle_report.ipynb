{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25185d83",
   "metadata": {},
   "source": [
    "# Report: wrangle_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e226be",
   "metadata": {},
   "source": [
    "The aim of this project is to carry out data wrangling. In this project, I need to gather data from the twitter user weratedogs. <br>\n",
    "For a data analyst to carry out an analysis, he or she needs the data to perform the task. One of the first steps in data analysis is the data gathering process.<br>\n",
    "<br>\n",
    "In this data Wrangling project, the first step was to import all python libraries (pandas, numpy, request, Jason, etc,.) that are needed to gather and save the data locally.\n",
    "Next step is, we start gathering three needed datasets.<br>\n",
    "<br>\n",
    "The first dataset twitter-archive-enhance.csv was downloaded manually and was read into pandas DataFrame. <br>\n",
    "Then the second dataset image_predictions_file was downloaded programmatically from the udacityâ€˜s servers, using the request library to get the data from a given link. <br>\n",
    "<br>\n",
    "For the third dataset, i had to use Twitter API to extract the favorite and retweet count. I had to setup Twitter developer account, apply to get the Twitter API keys and use them for data extraction on twitter. Getting this API keys took me about two days. The data was in an status object form, then we isolate the json part of each tweepy status object that we have downloaded and we save them all into a list. <br>\n",
    "<br>\n",
    "After manual and programmatically gathering the datasets, I began to access them. The assessment was done both visually first, and then programmatically. Quality and Tidiness issues were found in all three datasets and I documented this issues in an orderly manner, as this will be helpful when cleaning the data. <br>\n",
    "<br>\n",
    "The next step was the cleaning process. I carried out programmatic data cleaning throughout this process, making sure that every data cleaning steps was done using the earlier documented data Quality and Tidiness issues as a guide. <br>\n",
    "Lastly, after cleaning, I merged all three datasets together and saved them locally on my CPU as a csv file. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
